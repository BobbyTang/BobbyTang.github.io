<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Books | BobbyTang Blog]]></title>
  <link href="http://BobbyTang.github.io/blog/categories/books/atom.xml" rel="self"/>
  <link href="http://BobbyTang.github.io/"/>
  <updated>2014-05-30T08:40:42+08:00</updated>
  <id>http://BobbyTang.github.io/</id>
  <author>
    <name><![CDATA[BobbyTang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Java Performance - OS Monitoring]]></title>
    <link href="http://BobbyTang.github.io/blog/2014/05/21/java-performance-os-monitoring/"/>
    <updated>2014-05-21T00:16:37+08:00</updated>
    <id>http://BobbyTang.github.io/blog/2014/05/21/java-performance-os-monitoring</id>
    <content type="html"><![CDATA[<h2>Bottom Up Approach</h2>

<p>Bottom up begins at the lowest level of the software stack, at the CPU level looking at statistics such as CPU cache misses, inefficient use of CPU instructions, and then working up the software stack at what constructs or idioms are used by the application.</p>

<h2>Choosing the Right CPU Architecture</h2>

<p>One of the major design points behind the SPARC T-series processors is to address CPU cache misses by introducing multiple hardware threads per core.</p>

<h2>CPU Utilization</h2>

<p>A system with a single CPU socket with a quad core processor with hyperthreading disabled will show four CPUs in the GNOME System Monitor and report four virtual processors using the Java API Runtime.availableProcessors().</p>

<pre><code>xosview
vmstat
mpstat
top 
</code></pre>

<p>Which java thread is consuming CPU?</p>

<pre><code>jstack
</code></pre>

<h2>Monitoring Linux CPU Scheduler Run Queue</h2>

<pre><code>vmstat
</code></pre>

<h2>Memory Utilization</h2>

<pre><code>top
/proc/meminf
</code></pre>

<p>However, the following vmstat output from a Linux system illustrates a system that is experiencing swapping.  P36</p>

<h2>Monitoring Lock Contention on Linux</h2>

<pre><code>pidstat -w -I -p 9391 5
</code></pre>

<p>Hence, 3500 divided by 2, the num- ber of virtual processors = 1750. 1750 * 80,000 = 140,000,000. The number of clock cycles in 1 second on a 3.0GHz processor is 3,000,000,000. Thus, the percentage of clock cycles wasted on context switches is 140,000,000/3,000,000,000 = 4.7%.</p>

<p>The cost of a voluntary context switch at a processor clock cycle level is an expensive operation, generally upwards of about 80,000 clock cycles.</p>

<p>Again applying the general guideline of 3% to 5% of clock cycles spent in voluntary context switches implies a Java application that may be suffering from lock contention.</p>

<h2>Quick Lock Contention Monitoring</h2>

<h2>Isolating Hot Locks</h2>

<p>A common practice to find contended locks in a Java application has been to periodically take thread dumps and look for threads that tend to be blocked on the same lock across several thread dumps.</p>

<h2>Monitoring Involuntary Context Switches</h2>

<p>In contrast to voluntary context switching where an executing thread voluntarily takes itself off the CPU, involun- tary thread context switches occur when a thread is taken off the CPU as a result of an expiring time quantum or has been preempted by a higher priority thread.</p>

<p>Involuntary context switches can also be monitored on Linux using <code>pidstat -w</code>. High involuntary context switches are an indication there are more threads ready to run than there are virtual processors available to run them. As a result it is common to observe a high run queue depth in vmstat, high CPU utilization, and a high number of migrations (migrations are the next topic in this section) in conjunction with a large number of involuntary context switches.</p>

<p>On Linux, creation of processor sets and assigning applications to those processor sets can be accomplished using the Linux <code>taskset</code> command.</p>

<h2>Monitoring Thread Migrations</h2>

<p>As a general guideline, Java applications scaling across multiple cores or virtual processors and observing migrations greater than 500 per second could benefit from binding Java applications to processor sets.</p>

<h2>Network I/O Utilization</h2>

<pre><code>netstat -i 
nicstat
</code></pre>

<h2>Disk I/O Utilization</h2>

<pre><code>iostat -xm
</code></pre>

<p>One of the challenges with monitoring disk I/O utilization is identifying which files are being read or written to and which application is the source of the disk activity.</p>

<p>At the application level any strategy to minimize disk activity will help such as reducing the number of read and write operations using buffered input and output streams or integrating a caching data structure into the application to reduce or eliminate disk interaction.</p>

<h2>Additional Command Line Tools</h2>

<pre><code>sar
</code></pre>

<h2>Monitoring CPU Utilization on SPARC T-Series Systems</h2>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Operating System Concepts - IO System]]></title>
    <link href="http://BobbyTang.github.io/blog/2014/04/27/operating-system-concepts-io-system/"/>
    <updated>2014-04-27T17:43:57+08:00</updated>
    <id>http://BobbyTang.github.io/blog/2014/04/27/operating-system-concepts-io-system</id>
    <content type="html"><![CDATA[<h2>I/O Hardware</h2>

<p>The device communicates with the machine via a connection point, or <strong>port</strong> — for example, a serial port.</p>

<p>If devices use a common set of wires, the connection is called a bus. A <strong>bus</strong> is a set of wires and a rigidly defined protocol that specifies a set of messages that can be sent on the wires.</p>

<p>This figure shows a <strong>PCI bus</strong> (the common PC system bus) that connects the processor–memory subsystem to the fast devices and an expansion bus that connects relatively slow devices, such as the keyboard and serial and USB ports.</p>

<p><img class="<a" src="href="https://farm8.staticflickr.com/7406/14029529882_f49ab52235_z.jpg">https://farm8.staticflickr.com/7406/14029529882_f49ab52235_z.jpg</a>" title="pcbus_structure.png" ></p>

<p>A <strong>controller</strong> is a collection of electronics that can operate a port, a bus, or a device. A serial-port controller is a simple device controller. It is a single chip (or portion of a chip) in the computer that controls the signals on the wires of a serial port.</p>

<p>By contrast, a <strong>SCSI bus controller</strong> is not simple. Because the SCSI protocol is complex, the SCSI bus controller is often implemented as a separate circuit board (or a host adapter) that plugs into the computer. It typically contains a processor, microcode, and some private memory to enable it to process the SCSI protocol messages.</p>

<h2>Polling &amp; Interrupts</h2>

<p>In many computer architectures, three CPU-instruction cycles are sufficient to poll a device: read a device register, logical–and to extract a status bit, and branch if not zero. But <strong>polling</strong> becomes inefficient when it is attempted repeatedly yet rarely finds a device to be ready for service, while other useful CPU processing remains undone.</p>

<p>The hardware mechanism that enables a device to notify the CPU is called an <strong>interrupt</strong>.</p>

<p>The basic interrupt mechanism works as follows. The CPU hardware has a wire called the <strong>interrupt-request line</strong> that the CPU senses after executing every instruction. When the CPU detects that a controller has asserted a signal on the interrupt-request line, the CPU performs a state save and jumps to the <strong>interrupt-handler routine</strong> at a fixed address in memory.</p>

<p>Another example is found in the implementation of system calls. Usually, a program uses library calls to issue system calls. The library routines check the arguments given by the application, build a data structure to convey the arguments to the kernel, and then execute a special instruction called a <strong>software interrupt</strong>, or <strong>trap</strong>.</p>

<h2>Direct Memory Access</h2>

<p>For a device that does large transfers, such as a disk drive, it seems wasteful to use an expensive general-purpose processor to watch status bits and to feed data into a controller register one byte at a time—a process termed <strong>programmed I/O (PIO)</strong>. Many computers avoid burdening the main CPU with PIO by offloading some of this work to a special-purpose processor called a <strong>direct-memory-access (DMA) controller</strong>.</p>

<p><img class="<a" src="href="https://farm8.staticflickr.com/7340/14029558121_fb7debb6a5_z.jpg">https://farm8.staticflickr.com/7340/14029558121_fb7debb6a5_z.jpg</a>" title="DMA_transfer.png" ></p>

<h2>Blocking and Nonblocking I/O</h2>

<p>An alternative to a nonblocking system call is an asynchronous system call. An asynchronous call returns immediately, without waiting for the I/O to complete.</p>

<p>The difference between nonblocking and asynchronous system calls is that a nonblocking read() returns immediately with whatever data are available—the full number of bytes requested, fewer, or none at all. An asynchronous read() call requests a transfer that will be performed in its entirety but will complete at some future time.</p>

<p><img class="<a" src="href="https://farm3.staticflickr.com/2927/14032739105_b20504ca80_z.jpg">https://farm3.staticflickr.com/2927/14032739105_b20504ca80_z.jpg</a>" title="syn_asyn_io.png" ></p>

<h2>Buffering</h2>

<p>A <strong>buffer</strong> is a memory area that stores data being transferred between two devices or between a device and an application.</p>

<p>Buffering is done for three reasons.
1. One reason is to cope with a speed mismatch between the producer and consumer of a data stream.
2. A second use of buffering is to provide adaptations for devices that have different data-transfer sizes.
3. A third use of buffering is to support <strong>copy semantics</strong> for application I/O.</p>

<p>With <strong>copy semantics</strong>, the version of the data written to disk is guaranteed to be the version at the time of the application system call, independent of any subsequent changes in the application’s buffer. A simple way in which the operating system can guarantee copy semantics is for the write() system call to copy the application data into a kernel buffer before returning control to the application. The disk write is performed from the kernel buffer, so that subsequent changes to the application buffer have no effect.</p>

<h2>Caching</h2>

<p>A <strong>cache</strong> is a region of fast memory that holds copies of data.</p>

<p>When the kernel receives a file I/O request, the kernel first accesses the buffer cache to see whether that region of the file is already available in main memory. If it is, a physical disk I/O can be avoided or deferred.</p>

<h2>Transforming I/O Requests to Hardware Operations</h2>

<p>The figure suggests that an I/O operation requires a great many steps that together consume a tremendous number of CPU cycles.</p>

<ol>
<li>A process issues a blocking read() system call to a file descriptor of a file that has been opened previously.</li>
<li>The system-call code in the kernel checks the parameters for correctness. In the case of input, if the data are already available in the buffer cache, the data are returned to the process, and the I/O request is completed.</li>
<li>Otherwise, a physical I/O must be performed. The process is removed from the run queue and is placed on the wait queue for the device, and the I/O request is scheduled. Eventually, the I/O subsystem sends the request to the device driver. Depending on the operating system, the request is sent via a subroutine call or an in-kernel message.</li>
<li>The device driver allocates kernel buffer space to receive the data and schedules the I/O. Eventually, the driver sends commands to the device controller by writing into the device-control registers.</li>
<li>The device controller operates the device hardware to perform the data transfer.</li>
<li>The driver may poll for status and data, or it may have set up a DMA transfer into kernel memory. We assume that the transfer is managed by a DMA controller, which generates an interrupt when the transfer completes.</li>
<li>The correct interrupt handler receives the interrupt via the interrupt- vector table, stores any necessary data, signals the device driver, and returns from the interrupt.</li>
<li>The device driver receives the signal, determines which I/O request has completed, determines the request’s status, and signals the kernel I/O subsystem that the request has been completed.</li>
<li>The kernel transfers data or return codes to the address space of the requesting process and moves the process from the wait queue back to the ready queue.</li>
<li>Moving the process to the ready queue unblocks the process. When the scheduler assigns the process to the CPU, the process resumes execution at the completion of the system call.</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TODO Reading Book List]]></title>
    <link href="http://BobbyTang.github.io/blog/2014/04/23/todo-reading-book-list/"/>
    <updated>2014-04-23T06:39:31+08:00</updated>
    <id>http://BobbyTang.github.io/blog/2014/04/23/todo-reading-book-list</id>
    <content type="html"><![CDATA[<p>List the title of the books with corresponding reference url, for those which begin with */** are highly recommended.</p>

<h2>Fundamental</h2>

<ol>
<li>**<a href="http://www.amazon.com/Introduction-Algorithms-Thomas-H-Cormen/dp/0262033844/ref=sr_1_1?ie=UTF8&amp;qid=1398206561" title="amazon">Introduction to Algorithms</a></li>
<li>*<a href="http://www.amazon.com/Operating-System-Concepts-Abraham-Silberschatz/dp/047050949X/ref=sr_1_1?ie=UTF8&amp;qid=1398206626" title="amazon">Operating System Concepts with Java</a></li>
</ol>


<h2>Java</h2>

<ol>
<li>**<a href="http://www.amazon.com/Thinking-Java-Edition-Bruce-Eckel/dp/0131872486/ref=sr_1_1?ie=UTF8&amp;qid=1398207134" title="amazon">Thinking in Java (4th Edition)</a> prefer to starting with collections&amp;container chapter</li>
<li>*<a href="http://www.amazon.com/Java-Performance-Charlie-Hunt/dp/0137142528/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1398207290" title="amazon">Java Performance</a></li>
<li><a href="http://www.amazon.com/Java-Nio-Ron-Hitchens/dp/0596002882/ref=sr_1_1?ie=UTF8&amp;qid=1398207507" title="amazon">Java Nio</a></li>
<li><a href="http://www.amazon.com/Java-Message-Service-David-Chappell-ebook/dp/B0026OR3JY/ref=sr_1_1?ie=UTF8&amp;qid=1398207629" title="amazon">Java Message Service</a></li>
</ol>


<h2>Shell</h2>

<h2>DataBase</h2>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Operating System Concepts - VirtualMemory]]></title>
    <link href="http://BobbyTang.github.io/blog/2014/04/23/operating-system-concepts-virtualmemory/"/>
    <updated>2014-04-23T05:43:26+08:00</updated>
    <id>http://BobbyTang.github.io/blog/2014/04/23/operating-system-concepts-virtualmemory</id>
    <content type="html"><![CDATA[<h2>Demand Paging</h2>

<p>Loading the entire program into memory results in loading the executable code for all options, regardless of whether an option is ultimately selected by the user or not. An alternative strategy is to load pages only as they are needed. This technique is known as <strong>demand paging</strong> and is commonly used in virtual memory systems.</p>

<h2>Page Fault</h2>

<p>A page fault causes the following sequence to occur:</p>

<p><img class="<a" src="href="https://farm6.staticflickr.com/5325/13952117776_754a2b4bda.jpg">https://farm6.staticflickr.com/5325/13952117776_754a2b4bda.jpg</a>" title="Steps_in_handling_a_page_fault.png" ></p>

<ol>
<li>Trap to the operating system.</li>
<li>Save the user registers and process state.</li>
<li>Determine that the interrupt was a page fault.</li>
<li>Check that the page reference was legal and determine the location of the page on the disk.</li>
<li>Issue a read from the disk to a free frame:

<ol type="a">
<li>Wait in a queue for this device until the read request is serviced.</li>
<li>Wait for the device seek and/or latency time.</li>
<li>Begin the transfer of the page to a free frame.</li>
</ol>
</li>
<li>While waiting, allocate the CPU to some other user (CPU scheduling, optional).</li>
<li>Receive an interrupt from the disk I/O subsystem (I/O completed).</li>
<li>Save the registers and process state for the other user (if step 6 is executed).</li>
<li>Determine that the interrupt was from the disk.</li>
<li>Correct the page table and other tables to show that the desired page is now in memory.</li>
<li>Wait for the CPU to be allocated to this process again.</li>
<li>Restore the user registers, process state, and new page table, and then  resume the interrupted instruction.</li>
</ol>


<h2>Copy-on-Write</h2>

<p>Considering that many child processes invoke the exec() system call immediately after creation, the copying of the parent’s address space may be unnecessary.</p>

<p>Instead, we can use a technique known as <strong>copy-on-write</strong>, which works by allowing the parent and child processes initially to share the same pages. These shared pages are marked as copy-on-write pages, meaning that if either process writes to a shared page, a copy of the shared page is created.</p>

<h2>Page Replacement</h2>

<ol>
<li>Find the location of the desired page on the disk.</li>
<li>Find a free frame:

<ol type="a">
<li>If there is a free frame, use it.</li>
<li>If there is no free frame, use a page-replacement algorithm to select a victim frame.</li>
<li>Write the victim frame to the disk; change the page and frame tables accordingly.</li>
</ol>
</li>
<li>Read the desired page into the newly freed frame; change the page and frame tables.</li>
<li>Restart the user process.</li>
</ol>


<p><strong>Page Replacement Algorithm</strong></p>

<ul>
<li>FIFO Page Replacement</li>
<li>Optimal Page Replacement</li>
<li>LRU Page Replacement</li>
<li>LRU-Approximation Page Replacement</li>
<li>Additional-Reference-Bits Algorithm</li>
<li>Second-Chance Algorithm

<ol type="a">
<li> Enhanced Second-Chance Algorithm</li>
</ol>
</li>
<li>Counting-Based Page Replacement

<ol type="a">
<li> least-frequently-used (LFU) page-replacement algorithm</li>
<li> most-frequently-used (MFU) page-replacement algorithm</li>
</ol>
</li>
</ul>


<h2>Raw I/O</h2>

<p>Some operating systems give special programs the ability to use a disk partition as a large sequential array of logical blocks, without any file-system data structures. This array is sometimes called the raw disk, and I/O to this array is termed <strong>raw I/O</strong>.</p>

<p><strong>Raw I/O</strong> bypasses all the file- system services, such as file I/O demand paging, file locking, prefetching, space allocation, file names, and directories.</p>

<h2>Thrashing</h2>

<p>In fact, look at any process that does not have “enough” frames. If the process does not have the number of frames it needs to support pages in active use, it will quickly page-fault. At this point, it must replace some page. However, since all its pages are in active use, it must replace a page that will be needed again right away. Consequently, it quickly faults again, and again, and again, replacing pages that it must bring back in immediately.</p>

<p>This high paging activity is called <strong>thrashing</strong>. A process is thrashing if it is spending more time paging than executing.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Operating System Concepts - MainMemory]]></title>
    <link href="http://BobbyTang.github.io/blog/2014/04/09/operating-system-concepts-mainmemory/"/>
    <updated>2014-04-09T07:39:27+08:00</updated>
    <id>http://BobbyTang.github.io/blog/2014/04/09/operating-system-concepts-mainmemory</id>
    <content type="html"><![CDATA[<h2>Contiguous Memory Allocation</h2>

<p>Problem:</p>

<p>Both the first-fit and best-fit strategies for memory allocation suffer from <strong>external fragmentation</strong>.</p>

<p>The general approach to avoiding this problem is to break the physical memory into fixed-sized blocks and allocate memory in units based on block size. With this approach, the memory allocated to a process may be slightly larger than the requested memory. The difference between these two numbers is <strong>internal fragmentation</strong> — unused memory that is internal to a partition.</p>

<p>Solution:</p>

<p>One solution to the problem of external fragmentation is <strong>compaction</strong>. The goal is to shuffle the memory contents so as to place all free memory together in one large block. Compaction is possible only if relocation is dynamic and is done at execution time.</p>

<p>Another possible solution to the external-fragmentation problem is to permit the logical address space of the processes to be noncontiguous, thus allowing a process to be allocated physical memory wherever such memory is available.</p>

<h2>Page and Segmentation</h2>

<p><strong>Paging</strong> is a memory-management scheme that permits the physical address space of a process to be noncontiguous. Paging avoids external fragmentation and the need for compaction.</p>

<p><img class="<a" src="href="https://farm8.staticflickr.com/7454/13951470216_7af3187c5c.jpg">https://farm8.staticflickr.com/7454/13951470216_7af3187c5c.jpg</a>" title="paging_with_TLB" ></p>

<p>The <strong>translation look-aside buffer(TLB)</strong> is associative, high-speed memory. Each entry in the TLB consists of two parts: a key (or tag) and a value. When the associative memory is presented with an item, the item is compared with all keys simultaneously. If the item is found, the corresponding value field is returned.</p>

<p>If the page number is not in the TLB (known as a <strong>TLB miss</strong>), a memory reference to the page table must be made. When the frame number is obtained, we can use it to access memory (Figure 8.11). In addition, we add the page number and frame number to the TLB, so that they will be found quickly on the next reference.</p>

<p><strong>Segmentation</strong> is a memory-management scheme that supports this user view of memory.</p>

<h2>Page Table</h2>

<ul>
<li>hierarchical paging</li>
</ul>


<p>Problem&amp;Solution:</p>

<p>For example, consider a system with a 32-bit logical address space. If the page size in such a system is 4 KB (212), then a page table may consist of up to 1 million entries (232/212). Assuming that each entry consists of 4 bytes, each process may need up to 4 MB of physical address space for the page table alone. Clearly, we would not want to allocate the page table contiguously in main memory. One simple solution to this problem is to divide the page table into smaller pieces. We can accomplish this division in several ways.</p>

<pre><code>$ getconf PAGESIZE
4096
</code></pre>

<ul>
<li>hashed page tables</li>
</ul>


<p>Problem&amp;Solution:</p>

<p>A common approach for handling address spaces larger than 32 bits is to use a <strong>hashed page table</strong>, with the hash value being the virtual page number.</p>

<ul>
<li>inverted page tables</li>
</ul>


<p>Problem&amp;Solution:</p>

<p>Usually, each process has an associated page table. One of the drawbacks of this method is that each page table may consist of millions of entries. These tables may consume large amounts of physical memory just to keep track of how other physical memory is being used.</p>

<p>Drawbacks:</p>

<p>Although this scheme decreases the amount of memory needed to store each page table, it increases the amount of time needed to search the table when a page reference occurs.</p>

<p>Systems that use inverted page tables have difficulty implementing shared memory. Shared memory is usually implemented as multiple virtual addresses (one for each process sharing the memory) that are mapped to one physical address. This standard method cannot be used with inverted page tables; because there is only one virtual page entry for every physical page, one physical page cannot have two (or more) shared virtual addresses.</p>
]]></content>
  </entry>
  
</feed>
